{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "model = nn.Sequential(OrderedDict([('fc1',nn.Linear(784,128)),('relu1',nn.ReLU()),\n",
    "                                  ('fc2',nn.Linear(128,64)),('relu2',nn.ReLU()),\n",
    "                                  ('out',nn.Linear(64,10))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3007, grad_fn=<NllLossBackward>)\n",
      "torch.Size([128, 784])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# grab data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# flatten input images\n",
    "images = images.view(images.shape[0],-1)\n",
    "\n",
    "# forward pass\n",
    "logits = model(images)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "loss = criterion(logits,labels)\n",
    "\n",
    "print(loss)\n",
    "print(model.fc1.weight.shape)\n",
    "print(logits.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight.shape)\n",
    "print(model.fc2.weight.shape)\n",
    "print(model.out.weight.shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss. Note that for `nn.LogSoftmax` and `F.log_softmax` you'll need to set the `dim` keyword argument appropriately. `dim=0` calculates softmax across the rows, so each column sums to 1, while `dim=1` calculates across the columns so each row sums to 1. Think about what you want the output to be and choose `dim` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3374, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# neural network model\n",
    "model = nn.Sequential(nn.Linear(784,128),nn.ReLU(),nn.Linear(128,64),nn.ReLU(),nn.Linear(64,10),nn.LogSoftmax(dim=1))\n",
    "\n",
    "# defining the loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# data\n",
    "images,labels = next(dataiter)\n",
    "images = images.view(64,-1)\n",
    "\n",
    "# pass the input to the model and get logsoftmax (log-probabilities) output\n",
    "logps = model(images)\n",
    "\n",
    "# loss\n",
    "loss = criterion(logps,labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for real\n",
    "\n",
    "Now we'll put this algorithm along with __backward pass__ and __optimizer__ into a loop so we can go through all the images. Some nomenclature, one pass through the entire dataset is called an *epoch*. So here we're going to loop through `trainloader` to get our training batches. For each batch, we'll doing a training pass where we calculate the loss, do a backwards pass, and update the weights.\n",
    "\n",
    ">**Exercise:** Implement the training pass for our network. If you implemented it correctly, you should see the training loss drop with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8763343231764429\n",
      "0.835599205482489\n",
      "0.5264765585758793\n",
      "0.43489593140352\n",
      "0.38980008542601235\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# neural network model\n",
    "model = nn.Sequential(nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# defining the loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    \n",
    "    running_loss = 0  # keeps track of loss over the entire dataset\n",
    "    \n",
    "    for images,labels in trainloader:\n",
    "        # image flattening\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()   # because the gradient gets accumulated otherwise\n",
    "        \n",
    "        logps = model(images)   # forward pass\n",
    "        loss = criterion(logps,labels)   # loss calculation\n",
    "        loss.backward()   # backward pass : calculates the gradient of each model parameter w.r.t loss\n",
    "        optimizer.step()   # update the parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "      \n",
    "    print(running_loss/len(trainloader))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the loss decreases after every epoch, i.e. the network learns to make better predictions over the training data with each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking predictions of the trained network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images,labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0],1,-1)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(images[0])\n",
    "    \n",
    "# for getting probabilities, we take exponent of the log-probabilities\n",
    "ps = torch.exp(logps)\n",
    "               \n",
    "helper.view_classify(images[0].view(1,28,28),ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
